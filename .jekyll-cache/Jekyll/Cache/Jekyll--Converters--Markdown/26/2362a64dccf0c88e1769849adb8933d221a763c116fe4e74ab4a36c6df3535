I"-<body style="background-color: #142326"></body>

<p><a href="https://offensive-ai-lab.github.io/"><svg id="d3banner"></svg></a></p>

<script src="https://d3js.org/d3.v5.min.js"></script>

<script>
  // ported from https://bl.ocks.org/mbostock/3231298
  // adapted from https://github.com/columbiaviz/columbiaviz.github.io

window.mobileCheck = function() {
  let check = false;
  (function(a){if(/(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i.test(a)||/1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i.test(a.substr(0,4))) check = true;})(navigator.userAgent||navigator.vendor||window.opera);
  return check;
};



  var width =  document.getElementsByTagName("div")[0].offsetWidth ;
  var height = width > 500 ? 300 : 180;
  var strength = width > 500 ? 0.05 : 0.15;

  var numNodes = 200;
  var nodes = d3.range(numNodes).map(() => ({radius: Math.random() * 10+1}));
  var root = nodes[0],
      colorBad =  d3.scaleSequential(d3.interpolateOrRd),
	  colorGood =  d3.scaleSequential(d3.interpolateGreys);

  root.radius = 0;
  root.fixed = true;
  
  var toggle = -1;

  const svg = d3.select("#d3banner")
      .attr("width", width)
      .attr("height", height);

  const simulation = d3.forceSimulation(nodes)
      .force('charge', d3.forceManyBody().strength((d, i) => i ? 0 : -2000))
      .force('x', d3.forceX(width / 2).strength(strength))
      .force('y', d3.forceY(height / 2).strength(strength))
      .force('collision', d3.forceCollide().radius(d => d.radius))
      .on('tick', ticked);

  svg.selectAll("circle")
      .data(nodes.slice(1))
      .enter().append("circle")
      .attr("r", d => d.radius)
      .attr("opacity", 0.7)
      .style("fill", (d, i) => colorGood(i / (numNodes*2) +.2));

  svg.on("mousemove", function() {
      var p1 = d3.mouse(this);
      root.fx = p1[0];
      root.fy = p1[1];
      simulation
          .alphaTarget(0.3)
          .restart();
    });
	
  svg.on("touchmove", function() {
      var p1 = d3.mouse(this);
      root.fx = p1[0];
      root.fy = p1[1];
      simulation
          .alphaTarget(0.3)
          .restart();
    });

  function ticked() {
      svg.selectAll("circle")
          .attr("cx", d => d.x)
          .attr("cy", d => d.y);
  }
       
      root.fx = 100;
      root.fy = 100;
      simulation
          .alphaTarget(0.3)
          .restart();
  
 setInterval(function() 
 
 {
	toggle = toggle * -1;
	
	
	
    simulation.force('charge', d3.forceManyBody().strength((d, i) => i ? 0 : toggle*2000))
	
	if(toggle>0){
	svg.selectAll("circle")
      .data(nodes.slice(1))
      .style("fill", (d, i) => colorBad(i / (numNodes*2)+.5));
	}else{
	svg.selectAll("circle")
      .data(nodes.slice(1))
      .style("fill", (d, i) => colorGood(i / (numNodes*2)+.2));
	
	}
	
	
	}

	, 6000);

</script>

<p>Welcome to the course site for <code class="language-plaintext highlighter-rouge">Offensive AI</code> <script>document.write(new Date().getFullYear())</script> which I teach at BGU in the Department of Software and Information Systems Engineering.  Below you will find the course syllabus and more content will be added over the semester.</p>

<h3 id="syllabus">Syllabus</h3>

<p><strong>Course Name</strong>: Offensive AI<br />
 <strong>Course Name (Hebrew)</strong>: ×‘×™× ×” ××œ××›×•×ª×™×ª ×–×“×•× ×™×ª<br />
 <strong>Course Number</strong>: TBA<br />
 <strong>Course Structure</strong>: 3 hours of lectures weekly<br />
 <strong>Course Credits</strong>: 3<br />
 <strong>Lecturer</strong>: Dr. Yisroel Mirsky<br /></p>

<h4 id="course-description">Course Description:</h4>

<p>Artificial intelligence (AI) has provided us with the ability to automate tasks, extract information from vast amounts of data, and synthesize media that is nearly indistinguishable from the real thing. However, positive tools can also be used for negative purposes. In particular, cyber adversaries can also use AI, but to enhance their attacks and expand their campaigns.</p>

<p>In this course we will learn about attacks against AI systems (adversarial machine learning) such as model poisoning, model inversion, mebership inference, trojaning, and adversrial examples. We will also learn about attacks which use AI, such as deepfakes for facial reenactment and voice cloning, advanced spyware, autnomous bots, evasive malware, and the use of machine leanrning to detect software vulnerabilities. Finally, throughout the course we will learn how we can defend against these attacks and learn the best pratices for developing systems which are robust against them too.</p>

<h4 id="purpose-of-the-course">Purpose of the Course:</h4>

<p>The goal of the course is to learn (1) how AI is being used by malicious actors to exploit our AI systems and enhance their cyberattacks, and (2) how we can defend against these threats and develop safer systems.</p>

<h4 id="prerequisites">Prerequisites:</h4>

<p>At least one course in machine learning (e.g., 372.1.4951, 372.1.4952, 372.2.5910) or relevant experience in the subject. The course is open to students outside of the department on the basis of availability and faculty member recommendation.</p>

<h4 id="course-requirements">Course Requirements:</h4>

<ul>
  <li>Attendance is required.</li>
  <li>Students must learn the course from the lectures and any provided written materials.</li>
  <li>Students will submit one practical exercise in Python (10% of the grade), and one project which will be presented in the final lecture (15% of the grade).</li>
  <li>The final exam is 75% of the grade. Passing the exam is required for passing the course.</li>
</ul>

<h4 id="lectures">Lectures:</h4>

<p>(There may be small modifications)</p>

<p><code class="language-plaintext highlighter-rouge">Week 1</code> Introduction to machine learning and offensive AI.</p>

<p><em>Attacks on AI</em></p>

<p><code class="language-plaintext highlighter-rouge">Week 2</code> Adversarial Machine Learning I (Causative Attacks): <br />
Dataset poisoning and fault attacks (e.g., neural trojans, defense evasion, allergy attacks, clustering attacks).   <br /><code class="language-plaintext highlighter-rouge">Week 3</code> Adversarial Machine Learning II (Exploratory Attacks): <br />
Adversarial examples, sponge examples, model inversion, membership inference, and parameter inference. <br /><code class="language-plaintext highlighter-rouge">Week 4</code> Prevention and Mitigation of Adversarial Machine Learning <br /><code class="language-plaintext highlighter-rouge">Week 5</code> Lab: Adversarial Machine Learning with libraries and Torch in Python</p>

<p><em>Attacks using AI: Deepfakes</em></p>

<p><code class="language-plaintext highlighter-rouge">Week 6</code> Deepfakes I: <br />
Ethics of deepfakes and Generative AI used for facial reenactment  <br /><code class="language-plaintext highlighter-rouge">Week 7</code> Deepfakes II: <br />
Generative AI used for face replacement, face synthesis, and record tampering <br /><code class="language-plaintext highlighter-rouge">Week 8</code> Deepfakes III: <br />
Generative AI for voice cloning, spoofing, and audio driven reenactment  <br /><code class="language-plaintext highlighter-rouge">Week 9</code> Detection, Prevention, and Mitigation of Deepfakes <br /></p>

<p><em>Attacks using AI: Attack Tools</em></p>

<p><code class="language-plaintext highlighter-rouge">Week 10</code> Attack Planning and Exploit Development  <br /><code class="language-plaintext highlighter-rouge">Week 11</code> Spyware and Credential Theft <br /><code class="language-plaintext highlighter-rouge">Week 12</code> Intelligent Bots, Swarms, Detection Evasion, and Campaign Automation</p>

<p><em>Course Conclusion</em></p>

<p><code class="language-plaintext highlighter-rouge">Week 13</code> Student project presentations</p>

<h4 id="reading-list">Reading List:</h4>

<p>(tentative)</p>

<ol>
  <li>Huang, Ling, et al. â€œAdversarial machine learning.â€ <em>Proceedings of the 4th ACM workshop on Security and artificial intelligence</em>. 2011.</li>
  <li>Biggio, Battista, and Fabio Roli. â€œWild patterns: Ten years after the rise of adversarial machine learning.â€ <em>Pattern Recognition</em> 84 (2018): 317-331.</li>
  <li>Zhang, Jiliang, and Chen Li. â€œAdversarial examples: Opportunities and challenges.â€ <em>IEEE transactions on neural networks and learning systems</em> 31.7 (2019): 2578-2593.</li>
  <li>Carlini, Nicholas, et al. â€œOn evaluating adversarial robustness.â€ <em>arXiv preprint arXiv:1902.06705</em> (2019).</li>
  <li>Ilyas, Andrew, et al. â€œAdversarial examples are not bugs, they are features.â€ <em>arXiv preprint arXiv:1905.02175</em> (2019).</li>
  <li>Liu, Yuntao, et al. â€œA survey on neural trojans.â€ <em>2020 21st International Symposium on Quality Electronic Design (ISQED)</em>. IEEE, 2020.</li>
  <li>Chen, Huili, et al. â€œDeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks.â€ <em>IJCAI</em>. 2019.</li>
  <li>Mirsky, Yisroel, and Wenke Lee. â€œThe creation and detection of deepfakes: A survey.â€ <em>ACM Computing Surveys (CSUR)</em> 54.1 (2021): 1-41.</li>
  <li>Tolosana, Ruben, et al. â€œDeepfakes and beyond: A survey of face manipulation and fake detection.â€ <em>Information Fusion</em> 64 (2020): 131-148.</li>
  <li>Arik, Sercan O., et al. â€œNeural voice cloning with a few samples.â€ <em>arXiv preprint arXiv:1802.06006</em> (2018).</li>
  <li>Hettwer, Benjamin, Stefan Gehrer, and Tim GÃ¼neysu. â€œApplications of machine learning techniques in side-channel attacks: a survey.â€ <em>Journal of Cryptographic Engineering</em> 10.2 (2020): 135-162.</li>
  <li>Batina, Lejla, et al. â€œ{CSI}{NN}: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel.â€ <em>28th {USENIX} Security Symposium ({USENIX} Security 19)</em>. 2019.</li>
</ol>
:ET